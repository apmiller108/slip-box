:PROPERTIES:
:ID:       1608AF00-E150-468D-9387-B28A36DADEE0
:END:
#+title: Scrubbing data
#+date: 2023-09-29 13:30 PM
#+updated:  2023-09-29 16:00 PM
#+filetags: :postgres:

A strategy for scrubbing data from a postgres dump for use in a performance
testing environment.

* Identify which columns contain sensitive data
  Once identified, tag the columns with a column comment

  #+begin_src sql
    COMMENT ON COLUMN users.email IS 'sensitive_data=true'
  #+end_src

* Compose a function
  See also https://github.com/teoljungberg/fx for database function version
  management in Rails.

  Use a function to transform data while keeping it generally consistent
  (strings are the same length).
  See also https://www.postgresql.org/docs/current/functions-string.html for
  functions that are useful such as
  - CONCAT
  - SUBSTRING
  - MD5
  - RANDOM
  - GREATEST
  - SPLIT_PART

  Here is an example of an email scrubbing function. Built functions that are
  generally useful:

  #+begin_src sql
    CREATE OR REPLACE FUNCTION scrub_email(email_address varchar(255))
      RETURNS varchar(255) AS $$
      SELECT CONCAT(
        SUBSTRING(
          MD5(RANDOM()::TEXT),
          1,
          GREATEST(LENGTH(SPLIT_PART(email_address, '@', 1)), 5)
        ),
        '@',
        SPLIT_PART(email_address, '@', 2)
        );
      $$ LANGUAGE SQL;

    SELECT scrub_email(email) from users;
  #+end_src

* Table Copying
  Create a copy of a table. This will take the place of the original table
  and will contain the scrubbed columns.
** Copy table
  Create a copy of a table. This just copies the table schema. Not the data.
  See also https://www.postgresql.org/docs/current/sql-createtable.html#SQL-CREATETABLE-PARMS-LIKE
  #+begin_src sql
  CREATE TABLE users_copy (LIKE users INCLUDING ALL)
  #+end_src
*** Excluding things for speedier copying
  You can use ~EXCLUDING~ after ~INCLUDING ALL~
  (eg, ~LIKE users INCLUDING ALL EXCLUDING INDEXES~). Not having the indexes in
  place when the data is copied in the next step will be faster. That could be
  useful for millions of rows, but the indexes will still need to be created
  after the copying is done.
** Copy data
   Here custom scrubbing functions can be applied.
   #+begin_src sql
     INSERT INTO users_copy(first_name, last_name, email, type, created_at, updated_at)
                 (
                   SELECT first_name, last_name, scrub_email(email), type, created_at, updated_at
                   FROM users
                 )
   #+end_src
** Change sequence ownership
   If the primary key is generated by a sequence, that sequence is still owned
   by the original table it was copied from.

   #+begin_src sql
   ALTER SEQUENCE users_id_seq OWNED BY users_copy.id
   #+end_src
** Copy indexes and FKs
   **If the table was copied excluding Indexes and Constraints*, they will need to
   be copied over but with a slightly different name.

   To list the indexes on the original table:
   #+begin_src sql
   select pg_get_indexdef(indexrelid) || ';' AS index from pg_index where indrelid = 'public.users'::regclass;
   #+end_src
** Drop and rename
   Drop the original table and rename the copy in the same transaction. Cascade
   will drop related objects like views.
   #+begin_src sql
     BEGIN;
     DROP TABLE users CASCADE;
     ALTER TABLE users_copy RENAME TO users;
     COMMIT;
   #+end_src

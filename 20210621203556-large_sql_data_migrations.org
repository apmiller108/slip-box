#+title: Large SQL data migrations
#+date: 2021-06-21 20:35 PM
#+updated: 2021-06-25 10:21 AM
#+roam_tags: sql

* Use PL/pgSQL
  See [[file:20200121141321-sql_procedural_language.org][SQL Procedural Language]].

  One important caveat here is that we can't ~COMMIT~  transactions in PL/pgSQL.
  This becomes an issue when using ~LOOP~ on batched operations where it would
  be ideal to commit on each iteration of a loop. For this, we need to use
  [[https://www.postgresqltutorial.com/postgresql-stored-procedures/][PostgreSQL Stored Procedures]] or a function for older PG versions. See
  [[http://www.postgresql.cn/docs/11/plpgsql-transactions.html][plpgsql transaction management docs]].

  See also https://newbedev.com/postgresql-cannot-begin-end-transactions-in-pl-pgsql
  
* Drop and recreate indexes
* Use batches

  #+begin_src sql
    DO $$
    DECLARE
        row_count   integer := 0;
        batch_size  integer := 2;
        batch_start integer := 1;
        batch_end   integer := batch_size;
        affected    integer;

    BEGIN
        row_count := (SELECT count(*) FROM things);
        RAISE NOTICE '% things to update', row_count;

        WHILE row_count > 0
            LOOP

                -- DO STUFF LIKE INSERT OF UPDATE records
                -- UPDATE things...

                GET DIAGNOSTICS affected = ROW_COUNT;
                RAISE NOTICE '% things migrated', affected;

                batch_end := batch_end + batch_size;
                row_count := row_count - batch_size;

                RAISE NOTICE '% things remaining', row_count;
            END LOOP;
    END $$
  #+end_src
* Resources
  http://blog.plataformatec.com.br/2019/02/migrations-in-databases-with-large-amount-of-data/
  https://www.2ndquadrant.com/en/blog/7-best-practice-tips-for-postgresql-bulk-data-loading/
